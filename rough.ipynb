{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trafficvidset(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame)\n",
    "            x_arr = [np.concatenate(frames,axis=2)]\n",
    "            y_arr = [(y[0])]\n",
    "            \n",
    "            break            \n",
    "        \n",
    "        y_arr = np.array(y_arr).astype(np.float32)\n",
    "        x_arr = np.array(x_arr).astype(np.float32)\n",
    "        width = x_arr.shape[2]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=3)\n",
    "        return np.transpose(x_arr[0],(2,0,1)),y_arr[0] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of clips - 6200\n",
      "torch.Size([16, 150, 240, 240])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38863649/ipykernel_125555/1083932503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrafficvidset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vid_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvid_fps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "dset = trafficvidset(train_vid_folder,audio_file,vid_fps,duration,clip_delta=5)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "for data in dloader:\n",
    "    print(data[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "def vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    clips = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "        last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "        vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "        vid_files.append(vid_tuple)\n",
    "        for i in range(0,last_second+1):\n",
    "            clips.append([i,vid_tuple])\n",
    "\n",
    "#     print('files read')\n",
    "    \n",
    "    while(1):\n",
    "        indices = np.random.permutation(list(range(len(clips))))\n",
    "        for k in range(0,len(indices),batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(min(batch_size,len(indices)-k)):\n",
    "#                 print(min(batch_size,len(indices)-k))\n",
    "                clip_index = indices[i+k]\n",
    "                vid_tuple = clips[clip_index][1]\n",
    "                vid_start_second =  clips[clip_index][0]\n",
    "                audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "                video_snippet_start_index = vid_start_second*vid_fps\n",
    "                video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "                y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "\n",
    "                if y.shape[0] < 1:\n",
    "                    continue\n",
    "\n",
    "                frames = []\n",
    "                for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                    retval,frame = vid_tuple[0].read()\n",
    "                    if not retval:                    \n",
    "                        print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    frames.append(frame)\n",
    "                x_batch.append(np.concatenate(frames,axis=2))\n",
    "                y_batch.append(y[0])\n",
    "            y_batch = np.array(y_batch).astype(np.float32)\n",
    "            x_batch = np.array(x_batch).astype(np.float32)\n",
    "            width = x_batch.shape[2]\n",
    "            x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "            # yield [np.array(x_batch),np.array(y_batch)]\n",
    "            yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "        print(\"epoch\")\n",
    "                \n",
    "\n",
    "#     while(1):\n",
    "#         indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "#         x_batch = []\n",
    "#         y_batch = []\n",
    "#         for i in indices:\n",
    "#             vid_tuple = vid_files[i]\n",
    "# #             last_second = int(vid_tuple[1]/vid_fps) - duration\n",
    "#             if last_second < 0:\n",
    "#                 continue\n",
    "\n",
    "#             # print(int(vid_tuple[1]/vid_fps))\n",
    "#             # print(last_second)\n",
    "#             vid_start_second =  np.random.randint(0,last_second+1)\n",
    "#             audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "#             video_snippet_start_index = vid_start_second*vid_fps\n",
    "#             video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "#             vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "\n",
    "# #             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "# #             y = df[df[0] == audio_start_tstamp][14]\n",
    "#             y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "\n",
    "# #             print(y.shape)\n",
    "#             if y.shape[0] < 1:\n",
    "# #             if len(y) < 1:\n",
    "#                 continue\n",
    "# #             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "#             frames = []\n",
    "#             for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "                \n",
    "#                 retval,frame = vid_tuple[0].read()\n",
    "#                 if not retval:                    \n",
    "#                     print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "#                 frames.append(frame)\n",
    "#             x_batch.append(np.concatenate(frames,axis=2))\n",
    "#             y_batch.append(y[0])\n",
    "#         y_batch = np.array(y_batch).astype(np.float32)\n",
    "#         x_batch = np.array(x_batch).astype(np.float32)\n",
    "#         width = x_batch.shape[2]\n",
    "#         x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "#         # yield [np.array(x_batch),np.array(y_batch)]\n",
    "#         yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file))   \n",
    "        vid_frame_counter = 0\n",
    "        if not vid_obj.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "            continue\n",
    "        vid_files.append([vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,vid_frame_counter])\n",
    "\n",
    "#     print('files read')\n",
    "\n",
    "    while(1):\n",
    "        \n",
    "        indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in indices:\n",
    "            vid_tuple = vid_files[i]\n",
    "            frame_counter = vid_tuple[3]\n",
    "            if (vid_tuple[1] - frame_counter) > duration*vid_fps:\n",
    "                frame_counter = 0\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_counter)\n",
    "                \n",
    "            vid_start_second =  int(frame_counter/vid_fps)\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "\n",
    "#             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "            y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "#             print(y)\n",
    "            if y.shape[0] < 1:\n",
    "#             if len(y) < 1:\n",
    "                continue\n",
    "#             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "            frames = []\n",
    "            for j in range(frame_counter,frame_counter+(vid_fps*duration)):\n",
    "                \n",
    "                retval,frame = vid_tuple[0].read()\n",
    "#                 frame_counter += 1\n",
    "                if not retval:                    \n",
    "                    print(retval,frame_counter,vid_tuple[1])\n",
    "                frames.append(frame)\n",
    "               \n",
    "            x_batch.append(np.concatenate(frames,axis=2))\n",
    "            y_batch.append(y[0])\n",
    "            vid_tuple[3] += vid_fps \n",
    "        y_batch = np.array(y_batch).astype(np.float32)\n",
    "        x_batch = np.array(x_batch).astype(np.float32)\n",
    "        width = x_batch.shape[2]\n",
    "        x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "        # yield [np.array(x_batch),np.array(y_batch)]\n",
    "        yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563fea0f7e40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([15, 150, 240, 240]) torch.Size([15, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([11, 150, 240, 240]) torch.Size([11, 7]) torch.float32\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/2152935189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/629517504.py\u001b[0m in \u001b[0;36mvid_dataloader\u001b[0;34m(vid_folder, audio_file, vid_fps, duration, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m# yield [np.array(x_batch),np.array(y_batch)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "\n",
    "for data in loader:\n",
    "    print(data[0].shape, data[1].shape,data[0].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "batch_size  = 100 \n",
    "dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55a72b706940] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of clips - 1387\n",
      "no. of clips - 7679\n"
     ]
    }
   ],
   "source": [
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "test_dset = trafficvidset(test_vid_folder,audio_file,vid_fps,duration)\n",
    "train_dset = trafficvidset(train_vid_folder,audio_file,vid_fps,duration,clip_delta=4)\n",
    "\n",
    "loader = DataLoader(train_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(test_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "# loader = vid_dataloader(train_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "# test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "in_channels = duration*vid_fps*3*2\n",
    "out_dimension = 32\n",
    "\n",
    "res = c2d.Resnet(in_channels,out_dimension).to(device)\n",
    "res.model.train()\n",
    "criterion_loss = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(res.model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train 29.867095947265625\n",
      "Validation 18.763875484466553\n",
      "50\n",
      "Train 29.365791664123535\n",
      "Validation 40.09738826751709\n",
      "100\n",
      "Train 24.811059646606445\n",
      "Validation 20.53251600265503\n",
      "150\n",
      "Train 25.244449768066406\n",
      "Validation 19.45361590385437\n",
      "200\n",
      "Train 27.631628799438477\n",
      "Validation 20.75678253173828\n",
      "250\n",
      "Train 25.9790878868103\n",
      "Validation 48.27507781982422\n",
      "300\n",
      "Train 24.2585862159729\n",
      "Validation 26.743857383728027\n",
      "350\n",
      "Train 27.565343189239503\n",
      "Validation 25.345038414001465\n",
      "400\n",
      "Train 25.890446128845216\n",
      "Validation 24.790374755859375\n",
      "450\n",
      "Train 24.697357921600343\n",
      "Validation 17.58067488670349\n",
      "epoch\n",
      "0\n",
      "Train 26.565811157226562\n",
      "Validation 20.46248197555542\n",
      "50\n",
      "Train 27.192783203125\n",
      "Validation 41.440839767456055\n",
      "100\n",
      "Train 27.59659429550171\n",
      "Validation 32.611820697784424\n",
      "150\n",
      "Train 27.72354965209961\n",
      "Validation 38.35366630554199\n",
      "200\n",
      "Train 22.457594242095947\n",
      "Validation 19.780050039291382\n",
      "250\n",
      "Train 26.29104290008545\n",
      "Validation 19.548136234283447\n",
      "300\n",
      "Train 26.134798336029053\n",
      "Validation 27.871220111846924\n",
      "350\n",
      "Train 26.463872776031494\n",
      "Validation 28.60276985168457\n",
      "400\n",
      "Train 25.36435220718384\n",
      "Validation 30.052474975585938\n",
      "450\n",
      "Train 25.602377395629883\n",
      "Validation 45.832786083221436\n",
      "epoch\n",
      "0\n",
      "Train 17.667850494384766\n",
      "Validation 27.428345203399658\n",
      "50\n",
      "Train 28.46034086227417\n",
      "Validation 17.836720943450928\n",
      "100\n",
      "Train 23.792134094238282\n",
      "Validation 32.20928192138672\n",
      "150\n",
      "Train 26.275229854583742\n",
      "Validation 26.383010864257812\n",
      "200\n",
      "Train 26.944751987457277\n",
      "Validation 20.833426475524902\n",
      "250\n",
      "Train 24.593108501434326\n",
      "Validation 26.424211502075195\n",
      "300\n",
      "Train 24.339026794433593\n",
      "Validation 34.690979957580566\n",
      "350\n",
      "Train 28.374755668640137\n",
      "Validation 24.156868934631348\n",
      "400\n",
      "Train 24.79958984375\n",
      "Validation 24.077925205230713\n",
      "450\n",
      "Train 27.01615560531616\n",
      "Validation 17.123114585876465\n",
      "epoch\n",
      "0\n",
      "Train 21.62379264831543\n"
     ]
    }
   ],
   "source": [
    "v_loss = []\n",
    "t_loss = []\n",
    "while(1):\n",
    "    res.model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for i, data in enumerate(loader):\n",
    "#         i+=1\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "#         print('model start')\n",
    "        res.model.zero_grad()\n",
    "        y_pred = res.model(x)\n",
    "        loss = criterion_loss(y_pred,y)\n",
    "#         print('model_end')\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not i%50:\n",
    "            print(i)\n",
    "            t_loss.append(np.mean(losses))\n",
    "            print('Train' + ' ' + str(t_loss[-1]))\n",
    "            losses = []\n",
    "            test_loader = DataLoader(test_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "            for j,data in enumerate(test_loader):                \n",
    "                x = data[0].to(device)\n",
    "                y = data[1].to(device)\n",
    "                res.model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                    y_pred = res.model(x)\n",
    "                    losses.append(criterion_loss(y_pred,y).item())\n",
    "#                     print('Evaluate' + ' ' + str(loss.item()))\n",
    "                if j == 3:        \n",
    "                    break\n",
    "            v_loss.append(np.mean(losses))\n",
    "            print('Validation' + ' ' + str(v_loss[-1]))\n",
    "            losses = []    \n",
    "            res.model.train()\n",
    "                    \n",
    "    print('epoch')\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.3670, 14.6446, 20.1131, 24.6436, 29.1910, 34.7048, 39.6444, 43.7930,\n",
      "         46.8338, 52.6740, 53.6185, 55.8550, 59.0919, 60.1943, 62.4122, 63.4136,\n",
      "         64.4053, 65.3156, 64.3676, 61.3923, 59.0562, 58.0792, 57.8750, 55.7629,\n",
      "         51.7262, 49.4777, 44.4349, 40.5693, 34.4035, 27.3291, 18.9803, 12.5272],\n",
      "        [11.6437, 14.7802, 19.7262, 24.3143, 28.6962, 34.3230, 39.1473, 43.2612,\n",
      "         46.3080, 51.9514, 53.0182, 55.2673, 58.3882, 59.4199, 61.7377, 62.8702,\n",
      "         64.0070, 65.0819, 64.2692, 61.3151, 59.0229, 57.9433, 57.5922, 55.3538,\n",
      "         51.4185, 49.1825, 44.2971, 40.4270, 34.4392, 27.4759, 19.2649, 13.0582],\n",
      "        [11.5743, 14.7191, 19.7172, 24.2840, 28.6738, 34.2699, 39.0946, 43.2008,\n",
      "         46.2371, 51.8888, 52.9368, 55.1774, 58.3044, 59.3416, 61.6399, 62.7523,\n",
      "         63.8676, 64.9177, 64.0912, 61.1429, 58.8519, 57.7882, 57.4578, 55.2413,\n",
      "         51.3051, 49.0740, 44.1836, 40.3249, 34.3322, 27.3760, 19.1720, 12.9518],\n",
      "        [11.8450, 14.9205, 19.6141, 24.2533, 28.5687, 34.2820, 39.0675, 43.1825,\n",
      "         46.2460, 51.8113, 52.9478, 55.2138, 58.2892, 59.2874, 61.6693, 62.8763,\n",
      "         64.0947, 65.2619, 64.5177, 61.5618, 59.2817, 58.1419, 57.7122, 55.3964,\n",
      "         51.4965, 49.2566, 44.4313, 40.5396, 34.6192, 27.6822, 19.5058, 13.4007],\n",
      "        [11.2771, 14.4131, 19.4944, 23.9607, 28.3265, 33.7868, 38.5644, 42.6089,\n",
      "         45.5902, 51.2061, 52.1941, 54.3915, 57.5012, 58.5420, 60.7674, 61.8187,\n",
      "         62.8670, 63.8434, 62.9859, 60.0837, 57.8188, 56.8096, 56.5303, 54.3959,\n",
      "         50.4957, 48.2998, 43.4446, 39.6566, 33.7107, 26.8429, 18.7383, 12.5484],\n",
      "        [12.0832, 15.2359, 20.0705, 24.8080, 29.2293, 35.0577, 39.9557, 44.1632,\n",
      "         47.2912, 52.9935, 54.1479, 56.4611, 59.6104, 60.6385, 63.0632, 64.2843,\n",
      "         65.5179, 66.6986, 65.9298, 62.9061, 60.5734, 59.4143, 58.9910, 56.6322,\n",
      "         52.6401, 50.3511, 45.4087, 41.4328, 35.3706, 28.2716, 19.9086, 13.6495],\n",
      "        [ 9.4204, 12.8235, 19.3185, 23.2675, 27.8450, 32.4837, 37.3069, 41.1490,\n",
      "         43.8613, 49.7617, 50.2224, 52.2054, 55.4695, 56.6780, 58.4000, 58.9173,\n",
      "         59.3578, 59.6779, 58.4135, 55.6565, 53.4304, 52.8562, 53.1131, 51.5808,\n",
      "         47.6398, 45.5590, 40.5424, 37.0675, 30.9425, 24.2232, 16.2726,  9.6983],\n",
      "        [12.2860, 15.3674, 19.9134, 24.7009, 29.0412, 34.9557, 39.8030, 44.0055,\n",
      "         47.1474, 52.7537, 53.9837, 56.3122, 59.4055, 60.3949, 62.8875, 64.1883,\n",
      "         65.5105, 66.7917, 66.0996, 63.0787, 60.7641, 59.5394, 59.0295, 56.5887,\n",
      "         52.6425, 50.3529, 45.4847, 41.4924, 35.5137, 28.4543, 20.1438, 14.0064],\n",
      "        [ 8.7774, 12.3130, 19.3267, 23.1606, 27.7985, 32.1634, 37.0397, 40.8048,\n",
      "         43.4104, 49.4937, 49.7221, 51.6402, 54.9732, 56.2431, 57.8332, 58.1626,\n",
      "         58.3380, 58.4261, 57.0078, 54.2919, 52.0975, 51.6675, 52.1161, 50.7862,\n",
      "         46.8348, 44.7666, 39.6812, 36.2927, 30.0633, 23.3581, 15.4397,  8.6884],\n",
      "        [11.7768, 14.9325, 19.8896, 24.5254, 28.9388, 34.6281, 39.4902, 43.6412,\n",
      "         46.7184, 52.4012, 53.4873, 55.7591, 58.9031, 59.9391, 62.2867, 63.4393,\n",
      "         64.5971, 65.6952, 64.8845, 61.9039, 59.5928, 58.4946, 58.1299, 55.8606,\n",
      "         51.8947, 49.6377, 44.7164, 40.8073, 34.7755, 27.7529, 19.4721, 13.2242],\n",
      "        [11.4427, 14.5001, 19.2819, 23.7839, 28.0572, 33.5865, 38.2994, 42.3273,\n",
      "         45.3141, 50.8187, 51.8775, 54.0846, 57.1293, 58.1294, 60.4141, 61.5410,\n",
      "         62.6744, 63.7467, 62.9662, 60.0750, 57.8342, 56.7657, 56.4002, 54.1919,\n",
      "         50.3476, 48.1584, 43.3904, 39.5985, 33.7513, 26.9435, 18.9132, 12.8619],\n",
      "        [11.9202, 15.0488, 19.8714, 24.5483, 28.9331, 34.6853, 39.5369, 43.6982,\n",
      "         46.7916, 52.4434, 53.5727, 55.8594, 58.9838, 60.0035, 62.3935, 63.5909,\n",
      "         64.7984, 65.9522, 65.1794, 62.1906, 59.8811, 58.7450, 58.3352, 56.0158,\n",
      "         52.0609, 49.7964, 44.8981, 40.9679, 34.9605, 27.9357, 19.6564, 13.4512],\n",
      "        [11.5331, 14.6185, 19.4539, 23.9924, 28.3057, 33.8785, 38.6339, 42.6964,\n",
      "         45.7073, 51.2635, 52.3291, 54.5535, 57.6264, 58.6372, 60.9384, 62.0715,\n",
      "         63.2097, 64.2877, 63.4980, 60.5820, 58.3210, 57.2447, 56.8814, 54.6572,\n",
      "         50.7782, 48.5703, 43.7583, 39.9339, 34.0346, 27.1660, 19.0653, 12.9570],\n",
      "        [13.1729, 16.3848, 20.8732, 26.0150, 30.4935, 36.8293, 41.9118, 46.3372,\n",
      "         49.6183, 55.5124, 56.8616, 59.3294, 62.5468, 63.5395, 66.2622, 67.7155,\n",
      "         69.1548, 70.6414, 69.9660, 66.7717, 64.3609, 62.9771, 62.3671, 59.7201,\n",
      "         55.6228, 53.1722, 48.1168, 43.8946, 37.6486, 30.2047, 21.5042, 15.1327],\n",
      "        [11.9004, 14.9699, 19.6230, 24.2794, 28.5889, 34.3270, 39.1125, 43.2348,\n",
      "         46.3057, 51.8649, 53.0165, 55.2892, 58.3608, 59.3541, 61.7520, 62.9739,\n",
      "         64.2097, 65.3959, 64.6633, 61.7024, 59.4216, 58.2681, 57.8236, 55.4895,\n",
      "         51.5902, 49.3464, 44.5246, 40.6234, 34.7061, 27.7628, 19.5809, 13.4843],\n",
      "        [ 8.9022, 12.3926, 19.2527, 23.1007, 27.7058, 32.1220, 36.9675, 40.7375,\n",
      "         43.3594, 49.3755, 49.6596, 51.5871, 54.8900, 56.1405, 57.7565, 58.1310,\n",
      "         58.3717, 58.5158, 57.1403, 54.4236, 52.2302, 51.7679, 52.1664, 50.7914,\n",
      "         46.8576, 44.7942, 39.7440, 36.3483, 30.1667, 23.4833, 15.5862,  8.9034]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    print(res(data[0].to(device)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_15916/3693431966.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\BREAL\\AppData\\Local\\Temp/ipykernel_15916/3693431966.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if file_list1[0][0] > file_list2[0][0]:\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def get_files_in_interval(strt,end,path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_time = int(time.mktime(datetime.strptime(file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        if file_time >= strt and file_time < end:\n",
    "            file_list.append([file_time,file,os.path.join(path,file)])\n",
    "    return file_list\n",
    "\n",
    "\n",
    "path1 ='G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch'\n",
    "path2 = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/iteris'  \n",
    "strt_time = 1649999413\n",
    "end_time = 1650137713\n",
    "fps = 5\n",
    "\n",
    "\n",
    "file_list1 = get_files_in_interval(strt_time,end_time,path1)\n",
    "file_list2 = get_files_in_interval(strt_time,end_time,path2)\n",
    "\n",
    "print(len(file_list1))\n",
    "print(len(file_list2))\n",
    "# while(len(file_list2) and len(file_list1)):\n",
    "#     if file_list1[0][0] > file_list2[0][0]:\n",
    "#         ref_list = file_list1\n",
    "#         scroll_list = file_list2\n",
    "#     else:\n",
    "#         ref_list = file_list2\n",
    "#         scroll_list = file_list1\n",
    "\n",
    "#     while(ref_list[0][0] >= scroll_list[0][0] and ref_list[0][0] < scroll_list[1][0]):\n",
    "\n",
    "while(len(file_list2) and len(file_list1)):\n",
    "    if file_list1[0][0] > file_list2[0][0]:\n",
    "        efile = file_list2[0]\n",
    "        lfile = file_list1[0]\n",
    "    else:        \n",
    "        efile = file_list1[0]\n",
    "        lfile = file_list2[0]\n",
    "\n",
    "\n",
    "    evid = cv2.VideoCapture(efile[2])\n",
    "    fps = evid.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(evid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    if (efile[0] + duration) > lfile[0]:\n",
    "        lvid = cv2.VideoCapture(efile[2])\n",
    "        evid.set(cv2.CV_CAP_PROP_POS_FRAMES,fps*(lvid[0]-evid[0]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "\n",
    "# def test_func():\n",
    "#     duration = 5\n",
    "#     vid_fps = 5\n",
    "#     batch_size = 64\n",
    "#     vid_folder = 'G:/.shortcut-targets-by-id/1VaRYG8M-m7nfxKooARU6qTiHpVPhNHuV/out'\n",
    "#     audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "\n",
    "#     loader = vid_dataloader(vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "#     for data in loader:\n",
    "#         # print(data[0].shape, data[1].shape,data[0].dtype)\n",
    "#         break\n",
    "    \n",
    "\n",
    "# cProfile.run('test_func()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "# batch_size  = 100 \n",
    "# dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': True}\n",
    "\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
  },
  "kernelspec": {
   "display_name": "PyTorch-1.8.1",
   "language": "python",
   "name": "pytorch-1.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
