{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "def vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    clips = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "        last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "        vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "        vid_files.append(vid_tuple)\n",
    "        for i in range(0,last_second+1):\n",
    "            clips.append([i,vid_tuple])\n",
    "\n",
    "#     print('files read')\n",
    "    \n",
    "    while(1):\n",
    "        indices = np.random.permutation(list(range(len(clips))))\n",
    "        for k in range(0,len(indices),batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(min(batch_size,len(indices)-k)):\n",
    "#                 print(min(batch_size,len(indices)-k))\n",
    "                clip_index = indices[i+k]\n",
    "                vid_tuple = clips[clip_index][1]\n",
    "                vid_start_second =  clips[clip_index][0]\n",
    "                audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "                video_snippet_start_index = vid_start_second*vid_fps\n",
    "                video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "                y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "\n",
    "                if y.shape[0] < 1:\n",
    "                    continue\n",
    "\n",
    "                frames = []\n",
    "                for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                    retval,frame = vid_tuple[0].read()\n",
    "                    if not retval:                    \n",
    "                        print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    frames.append(frame)\n",
    "                x_batch.append(np.concatenate(frames,axis=2))\n",
    "                y_batch.append(y[0])\n",
    "            y_batch = np.array(y_batch).astype(np.float32)\n",
    "            x_batch = np.array(x_batch).astype(np.float32)\n",
    "            width = x_batch.shape[2]\n",
    "            x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "            # yield [np.array(x_batch),np.array(y_batch)]\n",
    "            yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "        print(\"epoch\")\n",
    "                \n",
    "\n",
    "#     while(1):\n",
    "#         indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "#         x_batch = []\n",
    "#         y_batch = []\n",
    "#         for i in indices:\n",
    "#             vid_tuple = vid_files[i]\n",
    "# #             last_second = int(vid_tuple[1]/vid_fps) - duration\n",
    "#             if last_second < 0:\n",
    "#                 continue\n",
    "\n",
    "#             # print(int(vid_tuple[1]/vid_fps))\n",
    "#             # print(last_second)\n",
    "#             vid_start_second =  np.random.randint(0,last_second+1)\n",
    "#             audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "#             video_snippet_start_index = vid_start_second*vid_fps\n",
    "#             video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "#             vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "\n",
    "# #             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "# #             y = df[df[0] == audio_start_tstamp][14]\n",
    "#             y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "\n",
    "# #             print(y.shape)\n",
    "#             if y.shape[0] < 1:\n",
    "# #             if len(y) < 1:\n",
    "#                 continue\n",
    "# #             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "#             frames = []\n",
    "#             for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "                \n",
    "#                 retval,frame = vid_tuple[0].read()\n",
    "#                 if not retval:                    \n",
    "#                     print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "#                 frames.append(frame)\n",
    "#             x_batch.append(np.concatenate(frames,axis=2))\n",
    "#             y_batch.append(y[0])\n",
    "#         y_batch = np.array(y_batch).astype(np.float32)\n",
    "#         x_batch = np.array(x_batch).astype(np.float32)\n",
    "#         width = x_batch.shape[2]\n",
    "#         x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "#         # yield [np.array(x_batch),np.array(y_batch)]\n",
    "#         yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file))   \n",
    "        vid_frame_counter = 0\n",
    "        if not vid_obj.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "            continue\n",
    "        vid_files.append([vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,vid_frame_counter])\n",
    "\n",
    "#     print('files read')\n",
    "\n",
    "    while(1):\n",
    "        \n",
    "        indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in indices:\n",
    "            vid_tuple = vid_files[i]\n",
    "            frame_counter = vid_tuple[3]\n",
    "            if (vid_tuple[1] - frame_counter) > duration*vid_fps:\n",
    "                frame_counter = 0\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_counter)\n",
    "                \n",
    "            vid_start_second =  int(frame_counter/vid_fps)\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "\n",
    "#             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "            y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "#             print(y)\n",
    "            if y.shape[0] < 1:\n",
    "#             if len(y) < 1:\n",
    "                continue\n",
    "#             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "            frames = []\n",
    "            for j in range(frame_counter,frame_counter+(vid_fps*duration)):\n",
    "                \n",
    "                retval,frame = vid_tuple[0].read()\n",
    "#                 frame_counter += 1\n",
    "                if not retval:                    \n",
    "                    print(retval,frame_counter,vid_tuple[1])\n",
    "                frames.append(frame)\n",
    "               \n",
    "            x_batch.append(np.concatenate(frames,axis=2))\n",
    "            y_batch.append(y[0])\n",
    "            vid_tuple[3] += vid_fps \n",
    "        y_batch = np.array(y_batch).astype(np.float32)\n",
    "        x_batch = np.array(x_batch).astype(np.float32)\n",
    "        width = x_batch.shape[2]\n",
    "        x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "        # yield [np.array(x_batch),np.array(y_batch)]\n",
    "        yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563fea0f7e40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([15, 150, 240, 240]) torch.Size([15, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([11, 150, 240, 240]) torch.Size([11, 7]) torch.float32\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/2152935189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/629517504.py\u001b[0m in \u001b[0;36mvid_dataloader\u001b[0;34m(vid_folder, audio_file, vid_fps, duration, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m# yield [np.array(x_batch),np.array(y_batch)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "\n",
    "for data in loader:\n",
    "    print(data[0].shape, data[1].shape,data[0].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "batch_size  = 100 \n",
    "dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'padding' (position 5) must be tuple of ints, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_73534/502014562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         print('model start')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print('model_end')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'padding' (position 5) must be tuple of ints, not str"
     ]
    }
   ],
   "source": [
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "loader = vid_dataloader(train_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "in_channels = duration*vid_fps*3*2\n",
    "out_dimension = 7\n",
    "\n",
    "res = c2d.Resnet(in_channels,out_dimension).to(device)\n",
    "res.model.train()\n",
    "criterion_loss = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(res.model.parameters(), lr = 2e-4)\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    res.model.train()\n",
    "    losses = []\n",
    "    for data in loader:\n",
    "        i+=1\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "#         print('model start')\n",
    "        res.model.zero_grad()\n",
    "        y_pred = res.model(x)\n",
    "        loss = criterion_loss(y_pred,y)\n",
    "#         print('model_end')\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not i%50:\n",
    "            print('Train' + ' ' + str(np.mean(losses)))\n",
    "            losses = []\n",
    "            test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "            for data in test_loader:                \n",
    "                x = data[0].to(device)\n",
    "                y = data[1].to(device)\n",
    "                res.model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                    y_pred = res.model(x)\n",
    "                    loss = criterion_loss(y_pred,y)\n",
    "                    print('Evaluate' + ' ' + str(loss.item()))\n",
    "                break\n",
    "            res.model.train()\n",
    "                    \n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_15916/3693431966.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\BREAL\\AppData\\Local\\Temp/ipykernel_15916/3693431966.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if file_list1[0][0] > file_list2[0][0]:\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def get_files_in_interval(strt,end,path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_time = int(time.mktime(datetime.strptime(file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        if file_time >= strt and file_time < end:\n",
    "            file_list.append([file_time,file,os.path.join(path,file)])\n",
    "    return file_list\n",
    "\n",
    "\n",
    "path1 ='G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch'\n",
    "path2 = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/iteris'  \n",
    "strt_time = 1649999413\n",
    "end_time = 1650137713\n",
    "fps = 5\n",
    "\n",
    "\n",
    "file_list1 = get_files_in_interval(strt_time,end_time,path1)\n",
    "file_list2 = get_files_in_interval(strt_time,end_time,path2)\n",
    "\n",
    "print(len(file_list1))\n",
    "print(len(file_list2))\n",
    "# while(len(file_list2) and len(file_list1)):\n",
    "#     if file_list1[0][0] > file_list2[0][0]:\n",
    "#         ref_list = file_list1\n",
    "#         scroll_list = file_list2\n",
    "#     else:\n",
    "#         ref_list = file_list2\n",
    "#         scroll_list = file_list1\n",
    "\n",
    "#     while(ref_list[0][0] >= scroll_list[0][0] and ref_list[0][0] < scroll_list[1][0]):\n",
    "\n",
    "while(len(file_list2) and len(file_list1)):\n",
    "    if file_list1[0][0] > file_list2[0][0]:\n",
    "        efile = file_list2[0]\n",
    "        lfile = file_list1[0]\n",
    "    else:        \n",
    "        efile = file_list1[0]\n",
    "        lfile = file_list2[0]\n",
    "\n",
    "\n",
    "    evid = cv2.VideoCapture(efile[2])\n",
    "    fps = evid.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(evid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    if (efile[0] + duration) > lfile[0]:\n",
    "        lvid = cv2.VideoCapture(efile[2])\n",
    "        evid.set(cv2.CV_CAP_PROP_POS_FRAMES,fps*(lvid[0]-evid[0]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "\n",
    "# def test_func():\n",
    "#     duration = 5\n",
    "#     vid_fps = 5\n",
    "#     batch_size = 64\n",
    "#     vid_folder = 'G:/.shortcut-targets-by-id/1VaRYG8M-m7nfxKooARU6qTiHpVPhNHuV/out'\n",
    "#     audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "\n",
    "#     loader = vid_dataloader(vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "#     for data in loader:\n",
    "#         # print(data[0].shape, data[1].shape,data[0].dtype)\n",
    "#         break\n",
    "    \n",
    "\n",
    "# cProfile.run('test_func()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "# batch_size  = 100 \n",
    "# dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': True}\n",
    "\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
  },
  "kernelspec": {
   "display_name": "PyTorch-1.8.1",
   "language": "python",
   "name": "pytorch-1.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
