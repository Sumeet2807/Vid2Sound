{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "# !pip install opencv-python\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trafficvidset(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1,normalize_examples=320):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        self.norm_eg = 320\n",
    "        self.curr_eg = 0\n",
    "        self.running_norm = None\n",
    "        self.running_std = None\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame)\n",
    "#             x_arr = [np.concatenate(frames,axis=2)]\n",
    "#             y_arr = [(y[0])]\n",
    "            \n",
    "#             break            \n",
    "        \n",
    "#         y_arr = np.array(y_arr).astype(np.float32)\n",
    "#         x_arr = np.array(x_arr).astype(np.float32)\n",
    "#         width = x_arr.shape[2]\n",
    "#         x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=3)\n",
    "#         return np.transpose(x_arr[0],(2,0,1)),y_arr[0]\n",
    "            x_arr = np.concatenate(frames,axis=2).astype(np.float32)\n",
    "            \n",
    "            break \n",
    "        width = x_arr.shape[1]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=2)\n",
    "        return np.transpose(x_arr,(2,0,1)),y.astype(np.float32)[0] \n",
    "    \n",
    "\n",
    "class trafficvidset_optflow(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame[...,[0,2]])\n",
    "            x_arr = [np.concatenate(frames,axis=2)]\n",
    "            y_arr = [(y[0])]\n",
    "            \n",
    "            break            \n",
    "        \n",
    "        y_arr = np.array(y_arr).astype(np.float32)\n",
    "        x_arr = np.array(x_arr).astype(np.float32)\n",
    "        width = x_arr.shape[2]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=3)\n",
    "        return np.transpose(x_arr[0],(2,0,1)),y_arr[0] \n",
    "        \n",
    "\n",
    "        \n",
    "class trafficvidset(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1,normalize_clips=150,frames_per_clip=2):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "        metric_frames = []\n",
    "        for id in np.random.randint(0,len(self.clips),normalize_clips):\n",
    "            \n",
    "            vid_tuple = self.clips[id][1]\n",
    "            for i,frame_id in enumerate(np.random.randint(0,vid_tuple[0].get(cv2.CAP_PROP_FRAME_COUNT),frames_per_clip)):\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_id)\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                width = frame.shape[1]\n",
    "                if retval is True:                    \n",
    "                    metric_frames.append(frame) \n",
    "        metric_frames = np.array(metric_frames)\n",
    "        self.norm1 = np.mean(metric_frames[...,:int(width/2),:],(0,1,2),keepdims=True)\n",
    "        self.std1 = np.std(metric_frames[...,:int(width/2),:],(0,1,2),keepdims=True)\n",
    "        self.norm2 = np.mean(metric_frames[...,int(width/2):,:],(0,1,2),keepdims=True)\n",
    "        self.std2 = np.std(metric_frames[...,int(width/2):,:],(0,1,2),keepdims=True)\n",
    " \n",
    "            \n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    \n",
    "                frames.append(frame)\n",
    "            x_arr = np.concatenate(frames,axis=2).astype(np.float32)\n",
    "            \n",
    "            break \n",
    "        width = x_arr.shape[1]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=2)\n",
    "        return np.transpose(x_arr,(2,0,1)),y.astype(np.float32)[0]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "# audio_file = '/home/s.saini/data/1653167477-1651577468.csv'\n",
    "# train_vid_folder = '/blue/h.azad/s.saini/data/videos'\n",
    "# test_vid_folder = '/home/s.saini/data/Videos/optflow/test'\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "dset = trafficvidset(train_vid_folder,audio_file,vid_fps,duration,clip_delta=5)\n",
    "tset = trafficvidset(test_vid_folder,audio_file,vid_fps,duration)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "tloader = DataLoader(tset, batch_size=16, shuffle=True,num_workers=0) \n",
    "for i, data in enumerate(tloader):\n",
    "# for data in dloader:\n",
    "    print(data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of clips - 1384\n",
      "no. of clips - 31055\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/1653167477-1651577468.csv'\n",
    "train_vid_folder = '/blue/h.azad/s.saini/data/videos'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/optflow/test'\n",
    "cols = None\n",
    "test_dset = trafficvidset(test_vid_folder,audio_file,vid_fps,duration,cols=cols)\n",
    "train_dset = trafficvidset(train_vid_folder,audio_file,vid_fps,duration,cols=cols,clip_delta=4)\n",
    "\n",
    "loader = DataLoader(train_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(test_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "# loader = vid_dataloader(train_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "# test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "in_channels = duration*vid_fps*2*3\n",
    "out_dimension = 32\n",
    "\n",
    "res = c2d.Resnet(in_channels,out_dimension).to(device)#[10,15,15,12]\n",
    "# res.model.train()\n",
    "criterion_loss = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(res.model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train 1985.143798828125\n",
      "[  -1.6094079   -4.5146437   -8.614045   -25.659155   -41.326275\n",
      "  -88.88975    -81.40795    -80.88844    -52.20608    -40.87087\n",
      " -105.41285    -83.05992   -133.17926   -137.77327   -119.404884\n",
      " -147.99915   -120.47618   -130.72835   -132.69266   -160.33684\n",
      " -156.75934   -166.6955    -166.52518   -148.93886   -135.65538\n",
      "  -77.92169    -61.75796    -41.996468   -26.11232    -17.446608\n",
      "  -12.068918    -1.9822409]\n",
      "200\n",
      "Train 454.43415592193605\n",
      "[ -0.19869065  -0.36822605  -1.2475212   -2.756872    -5.7569127\n",
      "  -7.4303246  -13.254303   -10.257144   -12.745881   -18.956455\n",
      " -21.10823    -22.951708   -33.883343   -36.864017   -33.31846\n",
      " -31.278683   -28.896944   -29.827515   -26.90622    -24.450657\n",
      " -24.69064    -22.048666   -19.43655    -19.835896   -16.669313\n",
      " -11.217428    -8.479362    -5.5189877   -3.300562    -2.0782127\n",
      "  -1.0868933   -0.2834972 ]\n",
      "400\n",
      "Train 32.59372864246368\n",
      "[-0.00541949 -0.00417793  0.002675    0.00158006  0.00190622  0.0186621\n",
      "  0.00721669  0.01308048  0.01257312 -0.0162344  -0.01054049 -0.01173186\n",
      " -0.03735518 -0.04095531 -0.02906001 -0.00686133 -0.01085258 -0.02094448\n",
      " -0.02645791 -0.03011203 -0.03056479 -0.01867676 -0.0114578  -0.01559913\n",
      " -0.01100707 -0.00033391  0.00314027  0.00681078  0.00319964  0.00137931\n",
      "  0.00076532 -0.00307918]\n",
      "600\n",
      "Train 31.535401854515076\n",
      "[-0.00256002 -0.00325978  0.00209385  0.00760317  0.00765693  0.01198286\n",
      "  0.01605022  0.00613689  0.00458658 -0.01326478 -0.01191294 -0.00576794\n",
      " -0.03350008 -0.02841043 -0.01883173  0.00251913  0.00201148 -0.00664937\n",
      " -0.00979912 -0.01447499 -0.01629817 -0.00318778  0.00191063  0.00021738\n",
      "  0.00434071  0.00777805  0.01189041  0.01557958  0.0102939   0.00795078\n",
      "  0.00354791 -0.00444829]\n",
      "800\n",
      "Train 30.31552384376526\n",
      "[-0.00284922 -0.00204873  0.0020228   0.00286031 -0.00100422  0.01040417\n",
      "  0.0137645   0.0142591   0.00601131 -0.00405931 -0.00265586  0.0014618\n",
      " -0.02664769 -0.02380049 -0.01935863  0.01090229  0.0063777  -0.00043952\n",
      " -0.00114954 -0.00666583 -0.00803673  0.00293118  0.00698501  0.00207645\n",
      "  0.00163698  0.00433552  0.00582439  0.00626045  0.00604206  0.00188166\n",
      " -0.00375032 -0.0047282 ]\n",
      "1000\n",
      "Train 29.3454354429245\n",
      "[-0.0012387   0.00061786  0.00557584  0.02419484  0.03644508  0.04257315\n",
      "  0.04329491  0.04918486  0.04060346  0.02436674  0.02649182  0.03111655\n",
      "  0.00310415  0.02074987  0.02197379  0.05149388  0.04135239  0.03709489\n",
      "  0.04071808  0.02678049  0.02620238  0.03388345  0.03917664  0.03933859\n",
      "  0.03758907  0.03334552  0.03845596  0.03705925  0.02857649  0.01986092\n",
      "  0.01095951  0.0008927 ]\n",
      "1200\n",
      "Train 30.091830525398255\n",
      "[0.00585824 0.01074046 0.02034366 0.02805233 0.03049541 0.06018519\n",
      " 0.05254418 0.05713874 0.0540033  0.02477843 0.02854264 0.04399461\n",
      " 0.00951624 0.01418304 0.03358001 0.07434052 0.07567072 0.07995218\n",
      " 0.08292252 0.08195728 0.07928181 0.0666073  0.05932856 0.04631662\n",
      " 0.04714942 0.05150658 0.04855019 0.03932667 0.03650135 0.02548182\n",
      " 0.01050198 0.00738293]\n",
      "1400\n",
      "Train 29.722026844024658\n",
      "[0.00719297 0.01048553 0.01931703 0.03392971 0.04311621 0.06382835\n",
      " 0.06937504 0.0717926  0.07058698 0.02837336 0.04406315 0.05036569\n",
      " 0.01877749 0.02791196 0.04476488 0.08002001 0.08734602 0.08263206\n",
      " 0.08436996 0.07976627 0.07620025 0.06930906 0.06984949 0.06572694\n",
      " 0.06924421 0.06253135 0.06293887 0.0538494  0.04810834 0.03771609\n",
      " 0.02285576 0.01135993]\n",
      "1600\n",
      "Train 29.81574538230896\n",
      "[0.00012618 0.0026623  0.00652283 0.01467192 0.01834923 0.03687447\n",
      " 0.03562272 0.02032673 0.02817959 0.01150912 0.00958556 0.02466732\n",
      " 0.00059867 0.0065608  0.01250082 0.03813434 0.04075676 0.03496253\n",
      " 0.03534484 0.03299552 0.02631128 0.02132154 0.02713943 0.02261531\n",
      " 0.03443944 0.02737677 0.03184271 0.0287019  0.02393728 0.0195415\n",
      " 0.01259267 0.00308949]\n",
      "1800\n",
      "Train 31.53921501159668\n",
      "[-0.00017083  0.00090748  0.00828242  0.01609749  0.02908325  0.04567564\n",
      "  0.0453037   0.04316723  0.04186457  0.01912731  0.02907026  0.03833818\n",
      " -0.00190806  0.00995833  0.0189774   0.041816    0.04053557  0.03529561\n",
      "  0.03337181  0.02252591  0.01596528  0.02661484  0.03671932  0.03054249\n",
      "  0.03355229  0.03761482  0.03617686  0.03245193  0.02662456  0.02280712\n",
      "  0.0113833   0.00443172]\n",
      "epoch\n",
      "0\n",
      "Train 34.08016586303711\n",
      "[-0.0220623  -0.0769788  -0.03989911 -0.0275836   0.05538958  0.04762924\n",
      " -0.02205408 -0.00317657  0.04023355 -0.10881221 -0.01888108  0.11142999\n",
      " -0.28007996 -0.05009437  0.02512324 -0.04584551 -0.0795747  -0.074628\n",
      " -0.09108567 -0.08543217 -0.07043934 -0.09525251 -0.08351934 -0.01370633\n",
      "  0.04781902  0.05760151  0.03260785  0.06528902  0.05670178  0.02497566\n",
      "  0.00503641 -0.05901742]\n",
      "200\n",
      "Train 29.824001388549803\n",
      "[-0.00012016  0.00284982  0.0199976   0.02598345  0.03265584  0.04495823\n",
      "  0.0610885   0.05777466  0.0587247   0.02976501  0.02972341  0.0465793\n",
      "  0.00931239  0.0273838   0.03755701  0.06070024  0.06253797  0.0421353\n",
      "  0.03767812  0.02544528  0.01831716  0.03742886  0.04869407  0.04703677\n",
      "  0.05678844  0.05910313  0.05765784  0.05610353  0.04431993  0.03749907\n",
      "  0.02392519  0.00910932]\n",
      "400\n",
      "Train 29.914592814445495\n",
      "[ 0.00210214  0.00674361  0.01822275  0.02280813  0.02771372  0.04559112\n",
      "  0.05885673  0.0422684   0.03161222  0.00509089  0.02640688  0.02833945\n",
      " -0.01349998 -0.0081501   0.01561475  0.04997808  0.0450778   0.04111099\n",
      "  0.03770673  0.02327472  0.01917374  0.02499521  0.03657341  0.03532118\n",
      "  0.03676516  0.04099047  0.04494405  0.04104865  0.03051335  0.02601409\n",
      "  0.0084309  -0.0012331 ]\n",
      "600\n",
      "Train 31.728414850234984\n",
      "[-0.00050247  0.00170654  0.01808411  0.02717537  0.04066658  0.05681866\n",
      "  0.06060666  0.04865766  0.05528903  0.02804273  0.04201686  0.05194837\n",
      "  0.01055145  0.02890128  0.0400849   0.07034665  0.06992638  0.06239784\n",
      "  0.05584908  0.04644996  0.03782558  0.04753608  0.05567366  0.04645181\n",
      "  0.0435912   0.05240786  0.05125433  0.05074966  0.04367119  0.03699654\n",
      "  0.01899284  0.00580156]\n",
      "800\n",
      "Train 29.75959328174591\n",
      "[0.00780517 0.01151365 0.02680123 0.03948623 0.05796903 0.05752909\n",
      " 0.06884342 0.05792487 0.05363405 0.01758575 0.03530723 0.05089372\n",
      " 0.01797944 0.02078617 0.04622298 0.08391744 0.08655089 0.08217359\n",
      " 0.08139592 0.07078356 0.06319898 0.06924742 0.07655191 0.06743789\n",
      " 0.06607968 0.06700999 0.06663728 0.06220609 0.05403662 0.0447976\n",
      " 0.02844107 0.01075679]\n",
      "1000\n",
      "Train 29.471175484657287\n",
      "[0.00317138 0.00601733 0.02141827 0.03381258 0.04581636 0.06307155\n",
      " 0.05747819 0.07462168 0.05769628 0.03590059 0.04030234 0.0531401\n",
      " 0.0164513  0.02575487 0.04641068 0.0766983  0.07362723 0.0605011\n",
      " 0.05435783 0.04052401 0.03623259 0.05224234 0.06898546 0.06410694\n",
      " 0.0724709  0.07219124 0.06902176 0.06790972 0.05628461 0.048172\n",
      " 0.02653432 0.00987732]\n"
     ]
    }
   ],
   "source": [
    "v_loss = []\n",
    "t_loss = []\n",
    "while(1):\n",
    "    res.model.train()\n",
    "    losses = []\n",
    "    losses_freq = []\n",
    "    y_archive = []\n",
    "    for i, data in enumerate(loader):\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "#         print(y.shape)\n",
    "        res.model.zero_grad()\n",
    "        y_pred = res.model(x)\n",
    "#         print(y_pred.shape)\n",
    "        loss = criterion_loss(y_pred,y)\n",
    "        losses.append(loss.item())\n",
    "        losses_freq.append(np.array(torch.mean(torch.square(y-y_pred),axis=0).cpu().detach()))\n",
    "        y_archive.append(np.array(y.cpu().detach()))\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not i%200:\n",
    "            print(i)\n",
    "            losses_freq_mean = np.mean(np.array(losses_freq),axis=0)\n",
    "#             print(losses_freq_mean)\n",
    "            t_loss.append(np.mean(losses))\n",
    "            print('Train' + ' ' + str(t_loss[-1]))\n",
    "#################\n",
    "            y_epoch = np.concatenate(y_archive)\n",
    "            y_mean = np.mean(y_epoch,axis=0)\n",
    "            r2 = 1 - (losses_freq_mean / np.mean(np.square(y_epoch - y_mean),axis=0))\n",
    "            print(r2)\n",
    "            y_archive = []\n",
    "#####################        \n",
    "            losses = []\n",
    "            losses_freq = []\n",
    "#             test_loader = DataLoader(test_dset, batch_size=16, shuffle=True,num_workers=0)\n",
    "#             for j,data in enumerate(test_loader):\n",
    "#                 print(j)\n",
    "#                 x = data[0].to(device)\n",
    "#                 y = data[1].to(device)\n",
    "#                 res.model.eval()\n",
    "#                 with torch.no_grad():                    \n",
    "#                     y_pred = res.model(x)\n",
    "#                     losses.append(criterion_loss(y_pred,y).item())\n",
    "#                 if j == 3:        \n",
    "#                     break\n",
    "#             v_loss.append(np.mean(losses))\n",
    "#             print('Validation' + ' ' + str(v_loss[-1]))\n",
    "#             losses = []    \n",
    "            res.model.train()\n",
    "                    \n",
    "    print('epoch')\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(t_loss,list(range(len(t_loss))))\n",
    "sns.lineplot(v_loss,list(range(len(v_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "df = pd.read_csv(audio_file,header=None)\n",
    "for col in df.columns:\n",
    "    print(np.std(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def get_files_in_interval(strt,end,path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_time = int(time.mktime(datetime.strptime(file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        if file_time >= strt and file_time < end:\n",
    "            file_list.append([file_time,file,os.path.join(path,file)])\n",
    "    return file_list\n",
    "\n",
    "\n",
    "path1 ='G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch'\n",
    "path2 = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/iteris'  \n",
    "strt_time = 1649999413\n",
    "end_time = 1650137713\n",
    "fps = 5\n",
    "\n",
    "\n",
    "file_list1 = get_files_in_interval(strt_time,end_time,path1)\n",
    "file_list2 = get_files_in_interval(strt_time,end_time,path2)\n",
    "\n",
    "print(len(file_list1))\n",
    "print(len(file_list2))\n",
    "# while(len(file_list2) and len(file_list1)):\n",
    "#     if file_list1[0][0] > file_list2[0][0]:\n",
    "#         ref_list = file_list1\n",
    "#         scroll_list = file_list2\n",
    "#     else:\n",
    "#         ref_list = file_list2\n",
    "#         scroll_list = file_list1\n",
    "\n",
    "#     while(ref_list[0][0] >= scroll_list[0][0] and ref_list[0][0] < scroll_list[1][0]):\n",
    "\n",
    "while(len(file_list2) and len(file_list1)):\n",
    "    if file_list1[0][0] > file_list2[0][0]:\n",
    "        efile = file_list2[0]\n",
    "        lfile = file_list1[0]\n",
    "    else:        \n",
    "        efile = file_list1[0]\n",
    "        lfile = file_list2[0]\n",
    "\n",
    "\n",
    "    evid = cv2.VideoCapture(efile[2])\n",
    "    fps = evid.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(evid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    if (efile[0] + duration) > lfile[0]:\n",
    "        lvid = cv2.VideoCapture(efile[2])\n",
    "        evid.set(cv2.CV_CAP_PROP_POS_FRAMES,fps*(lvid[0]-evid[0]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "\n",
    "# def test_func():\n",
    "#     duration = 5\n",
    "#     vid_fps = 5\n",
    "#     batch_size = 64\n",
    "#     vid_folder = 'G:/.shortcut-targets-by-id/1VaRYG8M-m7nfxKooARU6qTiHpVPhNHuV/out'\n",
    "#     audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "\n",
    "#     loader = vid_dataloader(vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "#     for data in loader:\n",
    "#         # print(data[0].shape, data[1].shape,data[0].dtype)\n",
    "#         break\n",
    "    \n",
    "\n",
    "# cProfile.run('test_func()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "# batch_size  = 100 \n",
    "# dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': True}\n",
    "\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "df = pd.read_csv(audio_file,header=None)\n",
    "# strt_time = 1649862360\n",
    "# end_time = 1649862432\n",
    "\n",
    "\n",
    "strt_time = 1649862480\n",
    "end_time = 1649862490\n",
    "df = df[(df[0] < end_time) & (df[0] >= strt_time)]\n",
    "# for col in range(17,25):\n",
    "#     sns.displot(df[col])\n",
    "sns.displot(df[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "def vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    clips = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "        last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "        vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "        vid_files.append(vid_tuple)\n",
    "        for i in range(0,last_second+1):\n",
    "            clips.append([i,vid_tuple])\n",
    "\n",
    "#     print('files read')\n",
    "    \n",
    "    while(1):\n",
    "        indices = np.random.permutation(list(range(len(clips))))\n",
    "        for k in range(0,len(indices),batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(min(batch_size,len(indices)-k)):\n",
    "#                 print(min(batch_size,len(indices)-k))\n",
    "                clip_index = indices[i+k]\n",
    "                vid_tuple = clips[clip_index][1]\n",
    "                vid_start_second =  clips[clip_index][0]\n",
    "                audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "                video_snippet_start_index = vid_start_second*vid_fps\n",
    "                video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "                y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "\n",
    "                if y.shape[0] < 1:\n",
    "                    continue\n",
    "\n",
    "                frames = []\n",
    "                for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                    retval,frame = vid_tuple[0].read()\n",
    "                    if not retval:                    \n",
    "                        print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    frames.append(frame)\n",
    "                x_batch.append(np.concatenate(frames,axis=2))\n",
    "                y_batch.append(y[0])\n",
    "            y_batch = np.array(y_batch).astype(np.float32)\n",
    "            x_batch = np.array(x_batch).astype(np.float32)\n",
    "            width = x_batch.shape[2]\n",
    "            x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "            # yield [np.array(x_batch),np.array(y_batch)]\n",
    "            yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "        print(\"epoch\")\n",
    "                \n",
    "\n",
    "#     while(1):\n",
    "#         indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "#         x_batch = []\n",
    "#         y_batch = []\n",
    "#         for i in indices:\n",
    "#             vid_tuple = vid_files[i]\n",
    "# #             last_second = int(vid_tuple[1]/vid_fps) - duration\n",
    "#             if last_second < 0:\n",
    "#                 continue\n",
    "\n",
    "#             # print(int(vid_tuple[1]/vid_fps))\n",
    "#             # print(last_second)\n",
    "#             vid_start_second =  np.random.randint(0,last_second+1)\n",
    "#             audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "#             video_snippet_start_index = vid_start_second*vid_fps\n",
    "#             video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "#             vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "\n",
    "# #             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "# #             y = df[df[0] == audio_start_tstamp][14]\n",
    "#             y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "\n",
    "# #             print(y.shape)\n",
    "#             if y.shape[0] < 1:\n",
    "# #             if len(y) < 1:\n",
    "#                 continue\n",
    "# #             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "#             frames = []\n",
    "#             for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "                \n",
    "#                 retval,frame = vid_tuple[0].read()\n",
    "#                 if not retval:                    \n",
    "#                     print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "#                 frames.append(frame)\n",
    "#             x_batch.append(np.concatenate(frames,axis=2))\n",
    "#             y_batch.append(y[0])\n",
    "#         y_batch = np.array(y_batch).astype(np.float32)\n",
    "#         x_batch = np.array(x_batch).astype(np.float32)\n",
    "#         width = x_batch.shape[2]\n",
    "#         x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "#         # yield [np.array(x_batch),np.array(y_batch)]\n",
    "#         yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file))   \n",
    "        vid_frame_counter = 0\n",
    "        if not vid_obj.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "            continue\n",
    "        vid_files.append([vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,vid_frame_counter])\n",
    "\n",
    "#     print('files read')\n",
    "\n",
    "    while(1):\n",
    "        \n",
    "        indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in indices:\n",
    "            vid_tuple = vid_files[i]\n",
    "            frame_counter = vid_tuple[3]\n",
    "            if (vid_tuple[1] - frame_counter) > duration*vid_fps:\n",
    "                frame_counter = 0\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_counter)\n",
    "                \n",
    "            vid_start_second =  int(frame_counter/vid_fps)\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "\n",
    "#             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "            y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "#             print(y)\n",
    "            if y.shape[0] < 1:\n",
    "#             if len(y) < 1:\n",
    "                continue\n",
    "#             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "            frames = []\n",
    "            for j in range(frame_counter,frame_counter+(vid_fps*duration)):\n",
    "                \n",
    "                retval,frame = vid_tuple[0].read()\n",
    "#                 frame_counter += 1\n",
    "                if not retval:                    \n",
    "                    print(retval,frame_counter,vid_tuple[1])\n",
    "                frames.append(frame)\n",
    "               \n",
    "            x_batch.append(np.concatenate(frames,axis=2))\n",
    "            y_batch.append(y[0])\n",
    "            vid_tuple[3] += vid_fps \n",
    "        y_batch = np.array(y_batch).astype(np.float32)\n",
    "        x_batch = np.array(x_batch).astype(np.float32)\n",
    "        width = x_batch.shape[2]\n",
    "        x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "        # yield [np.array(x_batch),np.array(y_batch)]\n",
    "        yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# duration = 5\n",
    "# vid_fps = 5\n",
    "# batch_size = 16\n",
    "# audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "# train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "# test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "# loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "\n",
    "# for data in loader:\n",
    "#     print(data[0].shape, data[1].shape,data[0].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "# batch_size  = 100 \n",
    "# dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': True}\n",
    "\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import numpy as np\n",
    "\n",
    "width = 480\n",
    "height = 240\n",
    "in_filename = '/home/s.saini/data/Videos/train/2022-04-13_11-59-49.mp4'\n",
    "process1 = (\n",
    "    ffmpeg\n",
    "    .input(in_filename, loglevel='panic')\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    in_bytes = process1.stdout.read(width * height * 3)\n",
    "    if not in_bytes:\n",
    "        break\n",
    "    in_frame = (\n",
    "        np\n",
    "        .frombuffer(in_bytes, np.uint8)\n",
    "        .reshape([height, width, 3])        \n",
    "    \n",
    "    )\n",
    "    print(in_frame.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    print(data[0].shape)\n",
    "    break\n",
    "# print(torch.sum(data[0][0][0]))\n",
    "# print(torch.sum(data[0][0][1]))\n",
    "# print(torch.sum(data[0][0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(data[0][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
  },
  "kernelspec": {
   "display_name": "PyTorch-1.10",
   "language": "python",
   "name": "pytorch-1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
