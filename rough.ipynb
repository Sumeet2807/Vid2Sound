{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "from torch.utils.data import Dataset\n",
    "from torchnet.dataset.splitdataset import SplitDataset\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "(63, 25, 240, 480, 3)\n",
      "(63, 32)\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "\n",
    "vid_folder = 'G:/.shortcut-targets-by-id/1VaRYG8M-m7nfxKooARU6qTiHpVPhNHuV/out'\n",
    "audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "df = pd.read_csv(audio_file,header=None)\n",
    "duration = 5\n",
    "vid_files = []\n",
    "vid_fps = 5\n",
    "if not len(vid_files):\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file))    \n",
    "        vid_files.append([vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp])\n",
    "\n",
    "print('files read')\n",
    "batch_size = 64\n",
    "indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "x_batch = []\n",
    "y_batch = []\n",
    "for i in indices:\n",
    "    vid_tuple = vid_files[i]\n",
    "    last_second = int(vid_tuple[1]/vid_fps) - duration + 1\n",
    "    if last_second < 0:\n",
    "        continue\n",
    "\n",
    "    # print(int(vid_tuple[1]/vid_fps))\n",
    "    # print(last_second)\n",
    "    vid_start_second =  np.random.randint(0,last_second+1)\n",
    "    audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "    video_snippet_start_index = vid_start_second*vid_fps\n",
    "    video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "    vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "\n",
    "    y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "    if y.shape[0] < 1:\n",
    "        continue\n",
    "\n",
    "    frames = []\n",
    "    for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "        \n",
    "        _,frame = vid_tuple[0].read()\n",
    "        # print(frame.shape)\n",
    "        frames.append(frame)\n",
    "    x_batch.append(frames)\n",
    "    y_batch.append(y[0])\n",
    "    print(len(y_batch))\n",
    "\n",
    "print(np.array(x_batch).shape)\n",
    "print(np.array(y_batch).shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VideoCapture 00000252271AE2B0>, 1499.0, 1649878188] 1649878463\n"
     ]
    }
   ],
   "source": [
    "index = 74\n",
    "vid_files[indices[index]]\n",
    "img = Image.fromarray(x_batch[index][0])\n",
    "img.show()\n",
    "print(vid_files[indices[index]],y_batch[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649876473"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "df = pd.read_csv(audio_file,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tstamp = 1649855472\n",
    "end_tstamp = 1649855482\n",
    "y = df[(df[0]>= start_tstamp) & (df[0] < end_tstamp)].drop(columns=[1]).to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "batch_size  = 100 \n",
    "dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2827272415161133\n",
      "1.6136571168899536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5712/2553218649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sumeet\\Desktop\\Projects\\Vid2Sound\\model\\c2d.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnnf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnnf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = c2d.Resnet(1,10,[3,1,1])\n",
    "res.model.train()\n",
    "criterion_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(res.model.parameters(), lr = 2e-4)\n",
    "\n",
    "while(1):\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "\n",
    "        res.model.zero_grad()\n",
    "\n",
    "        y_pred = res.model(x)\n",
    "        loss = criterion_loss(y_pred,y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not i%10:\n",
    "            print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((32,78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((240,240), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/*.png\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return {\"x\": img, \"y\": 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "dset = ImageDataset('C:\\\\Users\\\\Sumeet\\\\Desktop\\\\Projects\\\\Vid2Sound\\\\data\\\\training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = {0:0.25,1:0.25,2:0.25,3:0.25}\n",
    "split_dset = SplitDataset(dset,partitions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dset,\n",
    "    batch_size=2,\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dset.select(3)\n",
    "\n",
    "\n",
    "\n",
    "split_loader = DataLoader(\n",
    "    split_dset,\n",
    "    batch_size=2,\n",
    "    shuffle=True)\n",
    "\n",
    "# to_pil = transforms.ToPILImage()\n",
    "# count = 0\n",
    "# for data in split_loader:\n",
    "#     for i in range(data['x'].shape[0]):\n",
    "#         to_pil(data['x'][i]).show()\n",
    "#     count += 1\n",
    "\n",
    "# print(count)\n",
    "\n",
    "# count = 0\n",
    "# for data in train_loader:\n",
    "#     count += 1\n",
    "\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "0.66796875\n",
      "torch.Size([2])\n",
      "0.658203125\n",
      "torch.Size([2])\n",
      "0.65478515625\n",
      "torch.Size([2])\n",
      "0.6455078125\n",
      "torch.Size([1])\n",
      "0.6337890625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.c2d import Resnet\n",
    "model = Resnet(3,1,[1,1,1])\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for i,data in enumerate(split_loader):\n",
    "    optimizer.zero_grad()\n",
    "    x = data['x']\n",
    "    y = data['y'].type(torch.float16)\n",
    "    outputs = model(x)[:,0]\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11996/3509311955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0msecs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# batch.append(np.concatenate(frame,axis=2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "vid_capture = cv2.VideoCapture('test/2022-04-13_12-24-35.mp4')\n",
    "fps = 5\n",
    "batch = []\n",
    "frames = []\n",
    "secs = 0\n",
    "while(vid_capture.isOpened()):\n",
    "    ret, frame = vid_capture.read()\n",
    "    if ret == True:\n",
    "        frames.append(frame)\n",
    "        if len(frames) and not len(frames)%fps:\n",
    "            secs += 1\n",
    "            batch.append(np.concatenate(frame,axis=2))\n",
    "\n",
    "            frames = []\n",
    "    else:\n",
    "        vid_capture.release()\n",
    "        break\n",
    "\n",
    "vid_array = np.stack(batch)\n",
    "\n",
    "\n",
    "t = torch.Tensor(vid_array).type(torch.uint8)\n",
    "print(t.element_size()*t.nelement())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649867075"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "datetime_object = datetime.strptime('2022-04-13_12-24-35', '%Y-%m-%d_%H-%M-%S')\n",
    "int(time.mktime(datetime_object.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def get_files_in_interval(strt,end,path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_time = int(time.mktime(datetime.strptime(file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        if file_time >= strt and file_time < end:\n",
    "            file_list.append([file_time,file,os.path.join(path,file)])\n",
    "    return file_list\n",
    "\n",
    "\n",
    "path1 ='G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch'\n",
    "path2 = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/iteris'  \n",
    "strt_time = 1649999413\n",
    "end_time = 1650137713\n",
    "fps = 5\n",
    "\n",
    "\n",
    "file_list1 = get_files_in_interval(strt_time,end_time,path1)\n",
    "file_list2 = get_files_in_interval(strt_time,end_time,path2)\n",
    "\n",
    "print(len(file_list1))\n",
    "print(len(file_list2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1650137222,\n",
       "  '2022-04-16_15-27-02.mp4',\n",
       "  'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch\\\\2022-04-16_15-27-02.mp4'],\n",
       " [1650137522,\n",
       "  '2022-04-16_15-32-02.mp4',\n",
       "  'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch\\\\2022-04-16_15-32-02.mp4']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_15916/3693431966.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\BREAL\\AppData\\Local\\Temp/ipykernel_15916/3693431966.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if file_list1[0][0] > file_list2[0][0]:\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# while(len(file_list2) and len(file_list1)):\n",
    "#     if file_list1[0][0] > file_list2[0][0]:\n",
    "#         ref_list = file_list1\n",
    "#         scroll_list = file_list2\n",
    "#     else:\n",
    "#         ref_list = file_list2\n",
    "#         scroll_list = file_list1\n",
    "\n",
    "#     while(ref_list[0][0] >= scroll_list[0][0] and ref_list[0][0] < scroll_list[1][0]):\n",
    "\n",
    "while(len(file_list2) and len(file_list1)):\n",
    "    if file_list1[0][0] > file_list2[0][0]:\n",
    "        efile = file_list2[0]\n",
    "        lfile = file_list1[0]\n",
    "    else:        \n",
    "        efile = file_list1[0]\n",
    "        lfile = file_list2[0]\n",
    "\n",
    "\n",
    "    evid = cv2.VideoCapture(efile[2])\n",
    "    fps = evid.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(evid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    if (efile[0] + duration) > lfile[0]:\n",
    "        lvid = cv2.VideoCapture(efile[2])\n",
    "        evid.set(cv2.CV_CAP_PROP_POS_FRAMES,fps*(lvid[0]-evid[0]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
