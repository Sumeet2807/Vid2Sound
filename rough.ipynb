{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchsummary import summary\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trafficvidset(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame)\n",
    "            x_arr = [np.concatenate(frames,axis=2)]\n",
    "            y_arr = [(y[0])]\n",
    "            \n",
    "            break            \n",
    "        \n",
    "        y_arr = np.array(y_arr).astype(np.float32)\n",
    "        x_arr = np.array(x_arr).astype(np.float32)\n",
    "        width = x_arr.shape[2]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=3)\n",
    "            \n",
    "        return x_arr[0],y_arr[0] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635908b39c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n",
      "torch.Size([16, 240, 240, 150])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38577504/ipykernel_146370/61791542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrafficvidset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vid_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvid_fps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.8.1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/38577504/ipykernel_146370/3629009604.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_snippet_start_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo_snippet_end_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo_snippet_start_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvid_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvid_start_second\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_second\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "dset = trafficvidset(test_vid_folder,audio_file,vid_fps,duration)\n",
    "dloader = DataLoader(dset, batch_size=16, shuffle=True)\n",
    "for data in dloader:\n",
    "    print(data[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "def vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    clips = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "        last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "        vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "        vid_files.append(vid_tuple)\n",
    "        for i in range(0,last_second+1):\n",
    "            clips.append([i,vid_tuple])\n",
    "\n",
    "#     print('files read')\n",
    "    \n",
    "    while(1):\n",
    "        indices = np.random.permutation(list(range(len(clips))))\n",
    "        for k in range(0,len(indices),batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for i in range(min(batch_size,len(indices)-k)):\n",
    "#                 print(min(batch_size,len(indices)-k))\n",
    "                clip_index = indices[i+k]\n",
    "                vid_tuple = clips[clip_index][1]\n",
    "                vid_start_second =  clips[clip_index][0]\n",
    "                audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "                video_snippet_start_index = vid_start_second*vid_fps\n",
    "                video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "                y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "\n",
    "                if y.shape[0] < 1:\n",
    "                    continue\n",
    "\n",
    "                frames = []\n",
    "                for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                    retval,frame = vid_tuple[0].read()\n",
    "                    if not retval:                    \n",
    "                        print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    frames.append(frame)\n",
    "                x_batch.append(np.concatenate(frames,axis=2))\n",
    "                y_batch.append(y[0])\n",
    "            y_batch = np.array(y_batch).astype(np.float32)\n",
    "            x_batch = np.array(x_batch).astype(np.float32)\n",
    "            width = x_batch.shape[2]\n",
    "            x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "            # yield [np.array(x_batch),np.array(y_batch)]\n",
    "            yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "        print(\"epoch\")\n",
    "                \n",
    "\n",
    "#     while(1):\n",
    "#         indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "#         x_batch = []\n",
    "#         y_batch = []\n",
    "#         for i in indices:\n",
    "#             vid_tuple = vid_files[i]\n",
    "# #             last_second = int(vid_tuple[1]/vid_fps) - duration\n",
    "#             if last_second < 0:\n",
    "#                 continue\n",
    "\n",
    "#             # print(int(vid_tuple[1]/vid_fps))\n",
    "#             # print(last_second)\n",
    "#             vid_start_second =  np.random.randint(0,last_second+1)\n",
    "#             audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "#             video_snippet_start_index = vid_start_second*vid_fps\n",
    "#             video_snippet_end_index = video_snippet_start_index + (duration*vid_fps)\n",
    "#             vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "\n",
    "# #             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "# #             y = df[df[0] == audio_start_tstamp][14]\n",
    "#             y = df[df[0] == audio_start_tstamp][[14,15,16,17,18,19,20]].to_numpy()\n",
    "\n",
    "# #             print(y.shape)\n",
    "#             if y.shape[0] < 1:\n",
    "# #             if len(y) < 1:\n",
    "#                 continue\n",
    "# #             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "#             frames = []\n",
    "#             for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "                \n",
    "#                 retval,frame = vid_tuple[0].read()\n",
    "#                 if not retval:                    \n",
    "#                     print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "#                 frames.append(frame)\n",
    "#             x_batch.append(np.concatenate(frames,axis=2))\n",
    "#             y_batch.append(y[0])\n",
    "#         y_batch = np.array(y_batch).astype(np.float32)\n",
    "#         x_batch = np.array(x_batch).astype(np.float32)\n",
    "#         width = x_batch.shape[2]\n",
    "#         x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "#         # yield [np.array(x_batch),np.array(y_batch)]\n",
    "#         yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_vid_dataloader(vid_folder, audio_file, vid_fps, duration,batch_size):\n",
    "    df = pd.read_csv(audio_file,header=None)\n",
    "    vid_files = []\n",
    "    for vid_file in os.listdir(vid_folder):\n",
    "        if vid_file[-4:] != '.mp4':\n",
    "            continue\n",
    "        vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file))   \n",
    "        vid_frame_counter = 0\n",
    "        if not vid_obj.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "            continue\n",
    "        vid_files.append([vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,vid_frame_counter])\n",
    "\n",
    "#     print('files read')\n",
    "\n",
    "    while(1):\n",
    "        \n",
    "        indices = np.random.randint(0,len(vid_files),size=batch_size)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in indices:\n",
    "            vid_tuple = vid_files[i]\n",
    "            frame_counter = vid_tuple[3]\n",
    "            if (vid_tuple[1] - frame_counter) > duration*vid_fps:\n",
    "                frame_counter = 0\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_counter)\n",
    "                \n",
    "            vid_start_second =  int(frame_counter/vid_fps)\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "\n",
    "#             y = df[df[0] == audio_start_tstamp].drop(columns=[1]).to_numpy()\n",
    "            y = df[df[0] == audio_start_tstamp][[10,11,12,13,14,15,16,17,18,19,20,21,23,24,25]].to_numpy()\n",
    "#             print(y)\n",
    "            if y.shape[0] < 1:\n",
    "#             if len(y) < 1:\n",
    "                continue\n",
    "#             y = y.to_numpy()[np.newaxis,...]\n",
    "\n",
    "            frames = []\n",
    "            for j in range(frame_counter,frame_counter+(vid_fps*duration)):\n",
    "                \n",
    "                retval,frame = vid_tuple[0].read()\n",
    "#                 frame_counter += 1\n",
    "                if not retval:                    \n",
    "                    print(retval,frame_counter,vid_tuple[1])\n",
    "                frames.append(frame)\n",
    "               \n",
    "            x_batch.append(np.concatenate(frames,axis=2))\n",
    "            y_batch.append(y[0])\n",
    "            vid_tuple[3] += vid_fps \n",
    "        y_batch = np.array(y_batch).astype(np.float32)\n",
    "        x_batch = np.array(x_batch).astype(np.float32)\n",
    "        width = x_batch.shape[2]\n",
    "        x_batch = np.concatenate([x_batch[...,:int(width/2),:],x_batch[...,int(width/2):,:]],axis=3)\n",
    "        # yield [np.array(x_batch),np.array(y_batch)]\n",
    "        yield [torch.transpose(torch.tensor(x_batch),1,3),torch.tensor(y_batch)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563fea0f7e40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([15, 150, 240, 240]) torch.Size([15, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([11, 150, 240, 240]) torch.Size([11, 7]) torch.float32\n",
      "epoch\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n",
      "torch.Size([16, 150, 240, 240]) torch.Size([16, 7]) torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/2152935189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/38557191/ipykernel_100126/629517504.py\u001b[0m in \u001b[0;36mvid_dataloader\u001b[0;34m(vid_folder, audio_file, vid_fps, duration, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m# yield [np.array(x_batch),np.array(y_batch)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "\n",
    "for data in loader:\n",
    "    print(data[0].shape, data[1].shape,data[0].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "batch_size  = 100 \n",
    "dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train 2744.2598291015624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563553ea2880] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 1448.96240234375\n",
      "Train 538.4396939468384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563564348780] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 16.332712173461914\n",
      "Train 28.268496685028076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635655af640] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 95.98644256591797\n",
      "Train 23.5147114944458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635609992c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 27.311582565307617\n",
      "Train 20.412317485809325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355727c480] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 62.785560607910156\n",
      "Train 24.586294956207276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563565701c80] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 41.101051330566406\n",
      "Train 21.39602529525757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355a763540] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 35.10261535644531\n",
      "Train 20.120293273925782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635588b3100] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 12.238605499267578\n",
      "Train 21.099374561309816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56356e7fb0c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 24.454980850219727\n",
      "Train 18.541298809051515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355d9511c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 84.22341918945312\n",
      "Train 21.324986782073974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563595cf52c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 14.502190589904785\n",
      "Train 20.88283160209656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563556340800] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 35.73122787475586\n",
      "Train 21.064537353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355ead2cc0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 20.12318229675293\n",
      "Train 19.586722621917726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355b250440] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 33.12773513793945\n",
      "Train 22.820084953308104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563592fd4980] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 115.30316162109375\n",
      "Train 26.437195110321046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355ca834c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 29.836336135864258\n",
      "Train 24.728164291381837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563558e7db80] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 26.665687561035156\n",
      "Train 24.23441390991211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355ba7ac80] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 17.406723022460938\n",
      "Train 26.167094383239746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563550f3d3c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 74.11752319335938\n",
      "Train 23.571405181884767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355ea3c2c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 43.77870178222656\n",
      "Train 25.828923835754395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563562b4b340] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 63.642513275146484\n",
      "Train 23.08551224708557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56359622fb40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 41.73677062988281\n",
      "Train 21.649569664001465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635895c48c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 8.10752010345459\n",
      "Train 20.080559368133546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355fff9780] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 15.560524940490723\n",
      "Train 24.085115966796874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635adc5cd80] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 25.97366714477539\n",
      "Train 23.53103754043579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635547dd200] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 19.21333122253418\n",
      "Train 21.33851948738098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563552b579c0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 18.23131561279297\n",
      "Train 22.40235445022583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355f717c40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 15.746552467346191\n",
      "Train 22.41905872344971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563561c15a00] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 16.99588966369629\n",
      "Train 21.744329376220705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563560b21280] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 14.925396919250488\n",
      "Train 21.79306223869324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563564f0e880] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 11.622447967529297\n",
      "Train 19.091109371185304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355fa28040] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 24.50666618347168\n",
      "Train 19.820698175430298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56357057bfc0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 25.484600067138672\n",
      "Train 22.248351135253905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355223fb00] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 31.12067985534668\n",
      "Train 21.772375526428224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635649d6b80] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 30.00075912475586\n",
      "Train 21.65043095588684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355671c040] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 36.18827438354492\n",
      "Train 18.93524775505066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635651f0500] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 30.908004760742188\n",
      "Train 20.09950339317322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56354a323200] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 161.88824462890625\n",
      "epoch\n",
      "Train 21.28030309677124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563554b82980] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 36.236419677734375\n",
      "Train 19.111483306884764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635aeb86380] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 27.769569396972656\n",
      "Train 21.85668613433838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56356a616c00] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 17.411178588867188\n",
      "Train 21.28129566192627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563592836740] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 34.637794494628906\n",
      "Train 16.70250066757202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355c6c7880] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 23.772083282470703\n",
      "Train 18.344089031219482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56358beefec0] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 22.062835693359375\n",
      "Train 20.274671688079835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5635591e6b00] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 15.519227981567383\n",
      "Train 18.220735912322997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56359c682c00] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 12.434845924377441\n",
      "Train 17.361296186447145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x563558fbcf40] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 11.10804271697998\n",
      "Train 19.487152223587035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x56355e194800] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 14.391230583190918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/38577504/ipykernel_146370/2998187824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/38577504/ipykernel_146370/817337788.py\u001b[0m in \u001b[0;36mvid_dataloader\u001b[0;34m(vid_folder, audio_file, vid_fps, duration, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mvideo_snippet_start_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid_start_second\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvid_fps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mvideo_snippet_end_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_snippet_start_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvid_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mvid_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo_snippet_start_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maudio_start_tstamp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "duration = 5\n",
    "vid_fps = 5\n",
    "batch_size = 16\n",
    "audio_file = '/home/s.saini/data/Videos/audio/1649855468-1649880078.csv'\n",
    "train_vid_folder = '/home/s.saini/data/Videos/train'\n",
    "test_vid_folder = '/home/s.saini/data/Videos/test'\n",
    "\n",
    "loader = vid_dataloader(train_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "in_channels = duration*vid_fps*3*2\n",
    "out_dimension = 15\n",
    "\n",
    "res = c2d.Resnet(in_channels,out_dimension).to(device)\n",
    "res.model.train()\n",
    "criterion_loss = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(res.model.parameters(), lr = 2e-4)\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    res.model.train()\n",
    "    losses = []\n",
    "    for data in loader:\n",
    "        i+=1\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "#         print('model start')\n",
    "        res.model.zero_grad()\n",
    "        y_pred = res.model(x)\n",
    "        loss = criterion_loss(y_pred,y)\n",
    "#         print('model_end')\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not i%50:\n",
    "            print('Train' + ' ' + str(np.mean(losses)))\n",
    "            losses = []\n",
    "            test_loader = vid_dataloader(test_vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "            for data in test_loader:                \n",
    "                x = data[0].to(device)\n",
    "                y = data[1].to(device)\n",
    "                res.model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                    y_pred = res.model(x)\n",
    "                    loss = criterion_loss(y_pred,y)\n",
    "                    print('Evaluate' + ' ' + str(loss.item()))\n",
    "                break\n",
    "            res.model.train()\n",
    "                    \n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_15916/3693431966.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\BREAL\\AppData\\Local\\Temp/ipykernel_15916/3693431966.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if file_list1[0][0] > file_list2[0][0]:\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def get_files_in_interval(strt,end,path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_time = int(time.mktime(datetime.strptime(file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "        if file_time >= strt and file_time < end:\n",
    "            file_list.append([file_time,file,os.path.join(path,file)])\n",
    "    return file_list\n",
    "\n",
    "\n",
    "path1 ='G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/bosch'\n",
    "path2 = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/iteris'  \n",
    "strt_time = 1649999413\n",
    "end_time = 1650137713\n",
    "fps = 5\n",
    "\n",
    "\n",
    "file_list1 = get_files_in_interval(strt_time,end_time,path1)\n",
    "file_list2 = get_files_in_interval(strt_time,end_time,path2)\n",
    "\n",
    "print(len(file_list1))\n",
    "print(len(file_list2))\n",
    "# while(len(file_list2) and len(file_list1)):\n",
    "#     if file_list1[0][0] > file_list2[0][0]:\n",
    "#         ref_list = file_list1\n",
    "#         scroll_list = file_list2\n",
    "#     else:\n",
    "#         ref_list = file_list2\n",
    "#         scroll_list = file_list1\n",
    "\n",
    "#     while(ref_list[0][0] >= scroll_list[0][0] and ref_list[0][0] < scroll_list[1][0]):\n",
    "\n",
    "while(len(file_list2) and len(file_list1)):\n",
    "    if file_list1[0][0] > file_list2[0][0]:\n",
    "        efile = file_list2[0]\n",
    "        lfile = file_list1[0]\n",
    "    else:        \n",
    "        efile = file_list1[0]\n",
    "        lfile = file_list2[0]\n",
    "\n",
    "\n",
    "    evid = cv2.VideoCapture(efile[2])\n",
    "    fps = evid.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(evid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    if (efile[0] + duration) > lfile[0]:\n",
    "        lvid = cv2.VideoCapture(efile[2])\n",
    "        evid.set(cv2.CV_CAP_PROP_POS_FRAMES,fps*(lvid[0]-evid[0]))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "\n",
    "# def test_func():\n",
    "#     duration = 5\n",
    "#     vid_fps = 5\n",
    "#     batch_size = 64\n",
    "#     vid_folder = 'G:/.shortcut-targets-by-id/1VaRYG8M-m7nfxKooARU6qTiHpVPhNHuV/out'\n",
    "#     audio_file = 'G:/Shared drives/UF-AI-Catalyst/UF AI Code/test_data/1649855468-1649880078.csv'\n",
    "\n",
    "#     loader = vid_dataloader(vid_folder,audio_file,vid_fps,duration,batch_size)\n",
    "#     for data in loader:\n",
    "#         # print(data[0].shape, data[1].shape,data[0].dtype)\n",
    "#         break\n",
    "    \n",
    "\n",
    "# cProfile.run('test_func()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "# batch_size  = 100 \n",
    "# dataset = torchvision.datasets.MNIST(root='\\\\data',download=True,transform=transform)\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': True}\n",
    "\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4e011bea087348f089ed99a7f4744fa8e15b41a6914d8f39c20b970ed3434b0"
  },
  "kernelspec": {
   "display_name": "PyTorch-1.8.1",
   "language": "python",
   "name": "pytorch-1.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
