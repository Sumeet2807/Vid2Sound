{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "\n",
    "import torchvision\n",
    "import model.c2d as c2d\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class for generating video snippet samples from raw videos.\n",
    "class trafficvidset(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1,normalize_examples=320):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vid_folder: Folder containing videos with naming pattern as - YYYY-MM-DD_HH-MM-SS.mp4.\n",
    "                        example - 2022-04-13_12-14-33.mp4\n",
    "                        this is required for synchromizing audio levels from the csv to correct video frames\n",
    "            audio_file: A csv file where every row is a frequency domain audio level recorded for a particular \n",
    "                        unix timestamp\n",
    "            vid_fps: Frame-rate of the source videos\n",
    "            duration: duration of the snippets to be sampled\n",
    "            cols:Frequency channels to be included from the file. \"None\" means all frequency channels will be included\n",
    "            clip_delta: To strike a balance between having manageable sized data set and to span across almost all the recorded data,\n",
    "                        clip delta determines the time duration between two subsequent samples.\n",
    "            normalize_examples=320\n",
    "        Returns:\n",
    "            The decorated function will return the unbatched computation output Tensors.\n",
    "          \"\"\"\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        self.norm_eg = 320\n",
    "        self.curr_eg = 0\n",
    "        self.running_norm = None\n",
    "        self.running_std = None\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame)\n",
    "            x_arr = np.concatenate(frames,axis=2).astype(np.float32)\n",
    "            \n",
    "            break \n",
    "        width = x_arr.shape[1]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=2)\n",
    "        return np.transpose(x_arr,(2,0,1)),y.astype(np.float32)[0] \n",
    "    \n",
    "\n",
    "class trafficvidset_optflow(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                frames.append(frame[...,[0,2]])\n",
    "            x_arr = [np.concatenate(frames,axis=2)]\n",
    "            y_arr = [(y[0])]\n",
    "            \n",
    "            break            \n",
    "        \n",
    "        y_arr = np.array(y_arr).astype(np.float32)\n",
    "        x_arr = np.array(x_arr).astype(np.float32)\n",
    "        width = x_arr.shape[2]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=3)\n",
    "        return np.transpose(x_arr[0],(2,0,1)),y_arr[0] \n",
    "        \n",
    "\n",
    "        \n",
    "class trafficvidset_normalization(torch.utils.data.Dataset):\n",
    "    def __init__(self,vid_folder,audio_file, vid_fps, duration,cols=None,clip_delta=1,normalize_clips=150,frames_per_clip=2):\n",
    "        self.vid_fps = vid_fps\n",
    "        self.duration = duration\n",
    "        self.df = pd.read_csv(audio_file,header=None)\n",
    "        self.cols = cols\n",
    "        if self.cols is None:\n",
    "            self.cols = self.df.columns[1:] \n",
    "        vid_files = []\n",
    "        self.clips = []\n",
    "        for vid_file in os.listdir(vid_folder):\n",
    "            if vid_file[-4:] != '.mp4':\n",
    "                continue\n",
    "            vid_time_stamp = int(time.mktime(datetime.strptime(vid_file[0:-4], '%Y-%m-%d_%H-%M-%S').timetuple()))\n",
    "            vid_obj = cv2.VideoCapture(os.path.join(vid_folder,vid_file)) \n",
    "            last_second = int(vid_obj.get(cv2.CAP_PROP_FRAME_COUNT)/vid_fps) - duration \n",
    "            vid_tuple = [vid_obj,vid_obj.get(cv2.CAP_PROP_FRAME_COUNT),vid_time_stamp,last_second]\n",
    "            vid_files.append(vid_tuple)\n",
    "            for i in range(0,last_second+1,clip_delta):\n",
    "                self.clips.append([i,vid_tuple])\n",
    "        print(\"no. of clips - \" + str(len(self.clips)))\n",
    "        metric_frames = []\n",
    "        for id in np.random.randint(0,len(self.clips),normalize_clips):\n",
    "            \n",
    "            vid_tuple = self.clips[id][1]\n",
    "            for i,frame_id in enumerate(np.random.randint(0,vid_tuple[0].get(cv2.CAP_PROP_FRAME_COUNT),frames_per_clip)):\n",
    "                vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,frame_id)\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                width = frame.shape[1]\n",
    "                if retval is True:                    \n",
    "                    metric_frames.append(frame) \n",
    "        metric_frames = np.array(metric_frames)\n",
    "        self.norm1 = np.mean(metric_frames[...,:int(width/2),:],(0,1,2),keepdims=True)\n",
    "        self.std1 = np.std(metric_frames[...,:int(width/2),:],(0,1,2),keepdims=True)\n",
    "        self.norm2 = np.mean(metric_frames[...,int(width/2):,:],(0,1,2),keepdims=True)\n",
    "        self.std2 = np.std(metric_frames[...,int(width/2):,:],(0,1,2),keepdims=True)\n",
    " \n",
    "            \n",
    "                    \n",
    "    def __len__(self):\n",
    "        return(len(self.clips))\n",
    "    \n",
    "    def __getitem__(self,id):\n",
    "        \n",
    "        found = False        \n",
    "        while(not found):\n",
    "\n",
    "            vid_tuple = self.clips[id][1]\n",
    "            vid_start_second =  self.clips[id][0]\n",
    "            audio_start_tstamp = vid_tuple[2] + vid_start_second + math.ceil(duration/2)\n",
    "            video_snippet_start_index = vid_start_second*self.vid_fps\n",
    "            video_snippet_end_index = video_snippet_start_index + (self.duration*self.vid_fps)\n",
    "            vid_tuple[0].set(cv2.CAP_PROP_POS_FRAMES,video_snippet_start_index)\n",
    "            y = self.df[self.df[0] == audio_start_tstamp][self.cols].to_numpy()\n",
    "            \n",
    "            if y.shape[0] < 1:\n",
    "                if id < (len(self.clips)-1):\n",
    "                    id += 1\n",
    "                else:\n",
    "                    id = 0\n",
    "                continue\n",
    "\n",
    "            frames = []\n",
    "            for j in range(video_snippet_start_index,video_snippet_end_index):\n",
    "\n",
    "                retval,frame = vid_tuple[0].read()\n",
    "                if not retval:                    \n",
    "                    print(retval,video_snippet_start_index,j,vid_tuple[1],vid_start_second,last_second)\n",
    "                    \n",
    "                frames.append(frame)\n",
    "            x_arr = np.concatenate(frames,axis=2).astype(np.float32)\n",
    "            \n",
    "            break \n",
    "        width = x_arr.shape[1]\n",
    "        x_arr = np.concatenate([x_arr[...,:int(width/2),:],x_arr[...,int(width/2):,:]],axis=2)\n",
    "        return np.transpose(x_arr,(2,0,1)),y.astype(np.float32)[0]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
